---
sidebar_position: 2
---

# TinyLlama

*TinyLlama est un modèle de langage compact et open-source qui se distingue dans le monde de l’intelligence artificielle. Voici quelques points clés à propos de TinyLlama :*

1. ***Taille et Pré-entraînement** : TinyLlama est un modèle de langage de 1,1 milliard de paramètres23. Il a été pré-entraîné sur environ 1 billion de tokens pendant environ 3 époques.*

2. ***Architecture et Tokenizer** : TinyLlama est basé sur l’architecture et le tokenizer de Llama 2. Cela signifie que TinyLlama peut être intégré dans de nombreux projets open-source construits sur Llama.*

3. ***Efficacité Computationnelle** : TinyLlama utilise diverses avancées contribuées par la communauté open-source (par exemple, FlashAttention), ce qui lui permet d’atteindre une meilleure efficacité computationnelle.*

4. ***Performance** : Malgré sa taille relativement petite, TinyLlama démontre une performance remarquable dans une série de tâches en aval. Il surpasse significativement les modèles de langage open-source existants de tailles comparables.*

5. ***Légèreté** : La compacité de TinyLlama (seulement 1,1 milliard de paramètres) le rend idéal pour une multitude d’applications nécessitant une empreinte de calcul et de mémoire restreinte. C’est pourquoi TinyLlama est un excellent choix pour les appareils à mémoire limitée.*

Pour plus d’informations, vous pouvez consulter [le rapport technique](https://arxiv.org/abs/2401.02385) ou [le projet GitHub](https://github.com/jzhang38/TinyLlama) de TinyLlama.